# -*- coding: utf-8 -*-
"""COGS 118A Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1or4DnOX0kbMpjDnUozA9eVqdlN8vVQ7J
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Nov 25 12:32:47 2020

@author: aparn
"""


'''
HELPFUL LIBRARIES
numpy
scikit - this is where most of the things you need to implement is available
       - StandardScaler(), KFold(), SVM() ..... all models
       - auc(), fscore(), confusion_matrix()
matplotlib - plot() etc...
'''

import numpy as np

x = np. random.rand(5000,10)
y = np.random.randint(0,2,(5000,1)) # binary label

print(x.shape,y.shape)

n_folds = 5
n_per_fold = int(len(x)/n_folds)

fold_index = list(np.arange(5))

x_fold = []
y_fold = []

for i in fold_index:
    print(i*n_per_fold,(i*n_per_fold)+n_per_fold)
    x_fold.append(x[i*n_per_fold:(i*n_per_fold)+n_per_fold])
    y_fold.append(y[i*n_per_fold:(i*n_per_fold)+n_per_fold])
    
    
for i in range(n_folds):
    x_test = x_fold[i]
    y_test = y_fold[i]
    
    fold_index = list(np.arange(5))
    fold_index.remove(i)
    
    x_train = np.empty((0,10))
    y_train = np.empty((0,1))
    
    for j in fold_index:
        x_train = np.append(x_train,x_fold[j],0)
        y_train = np.append(y_train,y_fold[j],0)
        
    print('fold',i,'test',x_test.shape,y_test.shape,'train',x_train.shape,y_train.shape)
    
    
    #--------
    #normalize
    #---------
    
    
    #-----------------------------
    #BELOW IS A PSEUDOCODE FOR HOW THE REST OF THE PROJECT WOULD TYPICALLY FOLLOW
    #------------------------------
    
                #for each trial
    n_trials = 3
    for k in range(n_trials):
        
        #grid search for hyperparameters
        for p1 in para1:
            for p2 in para2:
                ;;;;
                ;;;;
                
                
                fitted_model = model(x_train,y_train,parameters = (p1,p2,...))
                pkl.dump(fitted model)
                
                #TIP: train and save your model with correct names; so that all you need to do is test later on
                
                
                #testing
                y_pred = model.predict(x_test)
                
                #compute metrics
                AUC, F-Score, Confusion #whatever metric you wish to report
                #use sklearn 
                
                
                #TIP: save everything! models in particular & the results so that you can generate results quickly

"""To clarify: 
For each dataset/algorithm combination do the following
Loop x3:
 - draw 5k random samples as training data, set aside rest for testing
 - GridsearchCV on training data to find the best hyperparameters
 - pick the best classifier, train on all training data (it had been fit on the last CV fold of from the gridsearch)
 - get test set error of this best classifier
 - record everything for you analysis
"""

import pandas as pd

#Reading the data and setting column headings
Adult_df = pd.read_csv('adult.data', names = ["age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss" , "hours-per-week", "native-country", "true-income-class"])
print(Adult_df.shape)


# One-Hot encoding on categorical columns 
Adult_df = pd.get_dummies(Adult_df, columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'])
print(Adult_df.shape)

#Data Cleaning 
Null_values = Adult_df.isnull().sum()
Adult_df.dropna()
Adult_df.drop_duplicates(inplace = True)
print(Adult_df.shape) #Looking at the change in shape, looks like there were some missing values and duplicates

#We split the y values from the data frame
y = Adult_df.loc[:,"true-income-class"]
# y = y_column.values
Adult_df = Adult_df.drop(labels="true-income-class", axis=1) #delete y column 
print(y.shape)
print(Adult_df.columns)

"""#Train / Test Split

Before we start with our analysis, we have to split the data into Test/Train splits. It can be done in two ways. Either we can randomly sample our data like we have been doing in class or we can use stratified sampling to make sure that we get an even representation from all classes. 
It would be interesting to see how much of a different these two different sampling techniques would make. 
"""

from collections import Counter
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# split the data into train and test set
test_fraction = 5000/Adult_df.shape[0] #We want test to be 5000 points to make sure it is consistent with CNM06.


#First we sample randomly
X_train, X_test, y_train, y_test = train_test_split(Adult_df, y, test_size=test_fraction, random_state=42) #random_state fixes the seed. 42 is just convention
print('Frequency of each class with just random sampling in Y Train is ', Counter(y_train))
print('Frequency of each class with just random sampling in Y Test is ',Counter(y_test))


X_train, X_test, y_train, y_test = train_test_split(Adult_df, y, test_size=test_fraction, random_state=42, stratify=y)
print('Frequency of each class with just random sampling in Y Train is ', Counter(y_train))
print('Frequency of each class with just random sampling in Y Train is ', Counter(y_test))

"""There is a clear difference between the splits with and without using Stratified Sampling. This makes sense as logically there would be more number of people that make less than \$50k as compared to the number of people that make more than \$50k. We want the most unbiased sample and hence we will be using Stratified Sampling to ensure the most balanced analysis.

#Model Analysis
"""

from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
from sklearn.model_selection import validation_curve
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns; sns.set_style('white')  # plot formatting



# Initializing Classifiers
clf1 = LogisticRegression(multi_class='multinomial',
                          solver='newton-cg',
                          random_state=12345)
clf2 = KNeighborsClassifier(algorithm='ball_tree',
                            leaf_size=50)
clf3 = SVC(random_state=12345)

# Building the pipelines
pipe1 = Pipeline([('std', StandardScaler()),
                  ('classifier', clf1)])

pipe2 = Pipeline([('std', StandardScaler()),
                  ('classifier', clf2)])

pipe3 = Pipeline([('std', StandardScaler()),
                  ('classifier', clf3)])


# Setting up the parameter grids
param_grid1 = [{'classifier__penalty': ['l2'],
                'classifier__C': np.power(10., np.arange(-4, 4))}] #Varying C

param_grid2 = [{'classifier__n_neighbors': list(range(1, 10)), #Varying K 
                'classifier__p': [1, 2]}]

param_grid3 = [{'classifier__kernel': ['rbf'],
                'classifier__C': np.power(10., np.arange(-4, 4)),
                'classifier__gamma': np.power(10., np.arange(-5, 0))},
               {'classifier__kernel': ['linear'],
                'classifier__C': np.power(10., np.arange(-4, 4))}]


# Setting up multiple GridSearchCV objects, 1 for each algorithm
gridcvs = {}

for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),
                            (pipe1, pipe2, pipe3),
                            ('Logistic', 'KNN', 'SVM')):
    gcv = GridSearchCV(estimator=est,
                       param_grid=pgrid,
                       scoring='accuracy',
                       n_jobs=1,
                       cv=2, # just 2-fold inner loop, i.e. train/test
                       verbose=0,
                       refit=True)
    gridcvs[name] = gcv

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# # ^^ this handy Jupyter magic times the execution of the cell for you
# 
# cv_scores = {name: [] for name, gs_est in gridcvs.items()}
# 
# skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)
# 
# # The outer loop for algorithm selection
# c = 1
# for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):
#     for name, gs_est in sorted(gridcvs.items()):
#         print('outer fold %d/5 | tuning %-8s' % (c, name), end='')
# 
#         # The inner loop for hyperparameter tuning
#         gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])
#         y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])
#         acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)
#         print(' | inner ACC %.2f%% | outer ACC %.2f%%' %
#               (gs_est.best_score_ * 100, acc * 100))
#         cv_scores[name].append(acc)
# 
#     c += 1
# 
#     #FYI: This code uses X_train.iloc[... ] instead of X_train[...] because the 
#     # penguin data is in a Dataframe instead of a numpy matrix
#

# Looking at the results
for name in cv_scores:
    print('%-8s | outer CV acc. %.2f%% +\- %.3f' % (
          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))
print()
for name in cv_scores:
    print('{} best parameters'.format(name), gridcvs[name].best_params_)

# Fitting a model to the whole training set
# using the "best" algorithm
best_algo = gridcvs['SVM']

best_algo.fit(X_train, y_train)
train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))
test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))

print('Accuracy %.2f%% (average over CV test folds)' %
      (100 * best_algo.best_score_))
print('Best Parameters: %s' % gridcvs['SVM'].best_params_)
print('Training Accuracy: %.2f%%' % (100 * train_acc))
print('Test Accuracy: %.2f%%' % (100 * test_acc))

best_algo

best_algo.cv_results_
# hmmm... how could you pull stuff out of these results to make some pretty graphs?